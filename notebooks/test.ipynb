{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from dialz.dataset import Dataset\n",
    "from dialz.model import ControlModel\n",
    "from dialz.vector import ControlVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>-6.52</td>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>1.54</td>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hate_speech_score  \\\n",
       "0  Yes indeed. She sort of reminds me of the elde...              -3.90   \n",
       "1  The trans women reading this tweet right now i...              -6.52   \n",
       "2  Question: These 4 broads who criticize America...               0.36   \n",
       "3  It is about time for all illegals to go back t...               0.26   \n",
       "4  For starters bend over the one in pink and kic...               1.54   \n",
       "\n",
       "   comment_id  annotator_id  \n",
       "0       47777         10873  \n",
       "1       39773          2790  \n",
       "2       47101          3379  \n",
       "3       43625          7365  \n",
       "4       12538           488  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hate Speech Dataset Load\n",
    "df = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
    "df = df[['text', 'hate_speech_score', 'comment_id', 'annotator_id']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 78427\n",
      "Number of label 0: 37856 (48.27%)\n",
      "Number of label 1: 40571 (51.73%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>-6.52</td>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>1.54</td>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sounds like the kinda wholsesome life I'd die ...</td>\n",
       "      <td>-4.93</td>\n",
       "      <td>11001</td>\n",
       "      <td>527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fuck off you insufferable retarded faggot.</td>\n",
       "      <td>2.08</td>\n",
       "      <td>17034</td>\n",
       "      <td>6801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hate_speech_score  \\\n",
       "0  Yes indeed. She sort of reminds me of the elde...              -3.90   \n",
       "1  The trans women reading this tweet right now i...              -6.52   \n",
       "4  For starters bend over the one in pink and kic...               1.54   \n",
       "5  Sounds like the kinda wholsesome life I'd die ...              -4.93   \n",
       "7         Fuck off you insufferable retarded faggot.               2.08   \n",
       "\n",
       "   comment_id  annotator_id  label  \n",
       "0       47777         10873      0  \n",
       "1       39773          2790      0  \n",
       "4       12538           488      1  \n",
       "5       11001           527      0  \n",
       "7       17034          6801      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column 'label' based on the hate speech score\n",
    "df['label'] = df['hate_speech_score'].apply(lambda x: 1 if x > 1 else (0 if x < -2 else None))\n",
    "\n",
    "# Remove rows without a label\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Print the head of the dataframe and the number of rows\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "label_counts = df['label'].value_counts()\n",
    "label_percentages = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Number of label 0: {label_counts[0]} ({label_percentages[0]:.2f}%)\")\n",
    "print(f\"Number of label 1: {label_counts[1]} ({label_percentages[1]:.2f}%)\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_dataset(\n",
    "        df: pd.DataFrame,\n",
    "        scoring_method: str,\n",
    "        model_name: str, \n",
    "        items: list, \n",
    "        prompt_type: str, \n",
    "        num_sents: int,\n",
    "        system_role: str,\n",
    "    ):\n",
    "    dataset = Dataset.create_dataset(model_name, items, prompt_type, num_sents, system_role)\n",
    "    \n",
    "    model = ControlModel(model_name, list(range(-5, -18, -1)), hf_token)\n",
    "    vector = ControlVector.train(model, dataset)\n",
    "\n",
    "    for i, text in tqdm(enumerate(df['text']), total=len(df), desc=\"Calculating activation scores\"):\n",
    "        df.at[i, 'activation_score'] = model.get_activation_score(text, vector, layer_index=list(range(15, 20, 1)), \n",
    "                                                                    scoring_method=scoring_method)\n",
    "    \n",
    "\n",
    "    def calculate_score(scores_df, label_0_condition, label_1_condition):\n",
    "        label_0_count = ((scores_df['label'] == 0) & label_0_condition).sum()\n",
    "        label_1_count = ((scores_df['label'] == 1) & label_1_condition).sum()\n",
    "        return label_0_count + label_1_count\n",
    "\n",
    "    # Generate a range of thresholds to test\n",
    "    thresholds = np.linspace(df['activation_score'].min(), df['activation_score'].max(), 1000)\n",
    "    condition = [calculate_score(df, df['activation_score'] < t, df['activation_score'] > t) for t in thresholds]\n",
    "\n",
    "    print(len(df))\n",
    "\n",
    "    best_threshold = thresholds[np.argmax(condition)]\n",
    "    accuracy_condition = (np.max(condition) / len(df)) * 100\n",
    "\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(f\"Accuracy: {accuracy_condition:.2f}%\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768ef30231a340ffbc49e10f2a5ebe52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m scores_df \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhate speech\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloving words\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenericqa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYou are an example of how someone would respond with \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mtest_dataset\u001b[0;34m(df, model_name, items, prompt_type, num_sents, system_role)\u001b[0m\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mcreate_dataset(model_name, items, prompt_type, num_sents, system_role)\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m ControlModel(model_name, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), hf_token)\n\u001b[0;32m---> 14\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mControlVector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating activation scores\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_activation_score(text, vector, layer_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[0;32m~/Work/dialz/dialz/vector.py:49\u001b[0m, in \u001b[0;36mControlVector.train\u001b[0;34m(cls, model, dataset, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 49\u001b[0m     dirs \u001b[38;5;241m=\u001b[39m \u001b[43mread_representations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(model_type\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_type, directions\u001b[38;5;241m=\u001b[39mdirs)\n",
      "File \u001b[0;32m~/Work/dialz/dialz/vector.py:203\u001b[0m, in \u001b[0;36mread_representations\u001b[0;34m(model, tokenizer, inputs, hidden_layers, batch_size, method, transform_hiddens)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# the order is [positive, negative, positive, negative, ...]\u001b[39;00m\n\u001b[1;32m    201\u001b[0m train_strs \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mentries \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (ex\u001b[38;5;241m.\u001b[39mpositive, ex\u001b[38;5;241m.\u001b[39mnegative)]\n\u001b[0;32m--> 203\u001b[0m layer_hiddens \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_get_hiddens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_strs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_hiddens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     layer_hiddens \u001b[38;5;241m=\u001b[39m transform_hiddens(layer_hiddens)\n",
      "File \u001b[0;32m~/Work/dialz/dialz/vector.py:286\u001b[0m, in \u001b[0;36mbatched_get_hiddens\u001b[0;34m(model, tokenizer, inputs, hidden_layers, batch_size)\u001b[0m\n\u001b[1;32m    284\u001b[0m encoded_batch \u001b[38;5;241m=\u001b[39m tokenizer(batch, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m encoded_batch \u001b[38;5;241m=\u001b[39m encoded_batch\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 286\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoded_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n",
      "File \u001b[0;32m~/Work/dialz/dialz/model.py:139\u001b[0m, in \u001b[0;36mControlModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:832\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 832\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:561\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    550\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    551\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    558\u001b[0m         position_embeddings,\n\u001b[1;32m    559\u001b[0m     )\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:246\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    185\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    195\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 196\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_2 = df.head(1000).reset_index(drop=True)\n",
    "\n",
    "print('Test 1')\n",
    "test1 = test_dataset(df_2, 'default', model_name, ['hate speech', 'loving words'], 'genericqa', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 2')\n",
    "test2 = test_dataset(df_2, 'default', model_name, ['hate speech', 'loving words'], 'starters', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 3')\n",
    "test3 = test_dataset(df_2, 'default', model_name, ['hate speech', 'loving words'], 'genericqa', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 4')\n",
    "test4 = test_dataset(df_2, 'default', model_name, ['hate speech', 'loving words'], 'genericqa', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 5')\n",
    "test5 = test_dataset(df_2, 'default', model_name, ['hate speech', 'loving words'], 'genericqa', 400, 'You are an example of how someone would respond with ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then test scoring_methods default, last_token, max_token, median_token\n",
    "# test alla the different layer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Best threshold: 0.22863410285285246\n",
      "Accuracy: 76.60%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# CONDITION 2 IS THE ONE I WANT\n",
    "\n",
    "# Define a function to calculate the score for a given threshold\n",
    "def calculate_score(scores_df, threshold, label_0_condition, label_1_condition):\n",
    "    label_0_count = ((scores_df['label'] == 0) & label_0_condition).sum()\n",
    "    label_1_count = ((scores_df['label'] == 1) & label_1_condition).sum()\n",
    "    return label_0_count + label_1_count\n",
    "\n",
    "# Generate a range of thresholds to test\n",
    "thresholds = np.linspace(scores_df['activation_score'].min(), scores_df['activation_score'].max(), 1000)\n",
    "scores_condition = [calculate_score(scores_df, t, scores_df['activation_score'] < t, scores_df['activation_score'] > t) for t in thresholds]\n",
    "\n",
    "print(len(scores_df))\n",
    "\n",
    "best_threshold = thresholds[np.argmax(scores_condition)]\n",
    "accuracy_condition = (np.max(scores_condition) / len(scores_df)) * 100\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Accuracy: {accuracy_condition:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 0.027358627319335936\n",
      "Max: 0.85576171875\n",
      "Min: -1.26796875\n",
      "Percentage of label 1s: 51.73%\n",
      "Percentage of label 0s: 48.27%\n"
     ]
    }
   ],
   "source": [
    "median = scores_df['activation_score'].median()\n",
    "max_value = scores_df['activation_score'].max()\n",
    "min_value = scores_df['activation_score'].min()\n",
    "\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Max: {max_value}\")\n",
    "print(f\"Min: {min_value}\")\n",
    "\n",
    "label_1_percentage = (scores_df['label'].value_counts(normalize=True)[1] * 100)\n",
    "label_0_percentage = (scores_df['label'].value_counts(normalize=True)[0] * 100)\n",
    "\n",
    "print(f\"Percentage of label 1s: {label_1_percentage:.2f}%\")\n",
    "print(f\"Percentage of label 0s: {label_0_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9743d05e9da54b56a8681b8e781a0449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:15<00:00,  1.60it/s]\n",
      "100%|██████████| 31/31 [00:03<00:00,  8.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.create_dataset(model_name, ['hate speech', 'loving words'], 'genericqa', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "model = ControlModel(model_name, list(range(-5, -18, -1)), hf_token)\n",
    "vector = ControlVector.train(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Activation visualization and scores for label 0 texts:\n",
      "Row 0:\n",
      "Text: Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \"Titanic\" who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!\n",
      "Activation Visualization:\n",
      "\u001b[38;2;228;228;255mYes\u001b[0m\u001b[38;2;0;0;255m indeed\u001b[0m\u001b[38;2;249;249;255m.\u001b[0m\u001b[38;2;252;252;255m She\u001b[0m\u001b[38;2;255;247;247m sort\u001b[0m\u001b[38;2;255;230;230m of\u001b[0m\u001b[38;2;255;228;228m reminds\u001b[0m\u001b[38;2;255;239;239m me\u001b[0m\u001b[38;2;228;228;255m of\u001b[0m\u001b[38;2;238;238;255m the\u001b[0m\u001b[38;2;187;187;255m elder\u001b[0m\u001b[38;2;236;236;255m lady\u001b[0m\u001b[38;2;209;209;255m that\u001b[0m\u001b[38;2;213;213;255m played\u001b[0m\u001b[38;2;225;225;255m the\u001b[0m\u001b[38;2;242;242;255m part\u001b[0m\u001b[38;2;237;237;255m in\u001b[0m\u001b[38;2;228;228;255m the\u001b[0m\u001b[38;2;236;236;255m movie\u001b[0m\u001b[38;2;220;220;255m \"\u001b[0m\u001b[38;2;255;243;243mT\u001b[0m\u001b[38;2;255;246;246mitan\u001b[0m\u001b[38;2;218;218;255mic\u001b[0m\u001b[38;2;215;215;255m\"\u001b[0m\u001b[38;2;223;223;255m who\u001b[0m\u001b[38;2;201;201;255m was\u001b[0m\u001b[38;2;216;216;255m telling\u001b[0m\u001b[38;2;223;223;255m her\u001b[0m\u001b[38;2;226;226;255m story\u001b[0m\u001b[38;2;241;241;255m!!!\u001b[0m\u001b[38;2;255;239;239m And\u001b[0m\u001b[38;2;230;230;255m I\u001b[0m\u001b[38;2;248;248;255m wouldn\u001b[0m\u001b[38;2;255;206;206m'\u001b[0m\u001b[38;2;255;228;228mt\u001b[0m\u001b[38;2;255;173;173m have\u001b[0m\u001b[38;2;252;252;255m wanted\u001b[0m\u001b[38;2;255;206;206m to\u001b[0m\u001b[38;2;255;219;219m cover\u001b[0m\u001b[38;2;255;216;216m who\u001b[0m\u001b[38;2;255;215;215m I\u001b[0m\u001b[38;2;255;238;238m really\u001b[0m\u001b[38;2;255;223;223m am\u001b[0m\u001b[38;2;237;237;255m!!\u001b[0m\u001b[38;2;222;222;255m I\u001b[0m\u001b[38;2;255;244;244m would\u001b[0m\u001b[38;2;255;201;201m be\u001b[0m\u001b[38;2;255;242;242m proud\u001b[0m\u001b[38;2;255;253;253m!!!!\u001b[0m\u001b[38;2;255;235;235m WE\u001b[0m\u001b[38;2;255;234;234m should\u001b[0m\u001b[38;2;255;212;212m be\u001b[0m\u001b[38;2;255;220;220m proud\u001b[0m\u001b[38;2;212;212;255m of\u001b[0m\u001b[38;2;215;215;255m our\u001b[0m\u001b[38;2;255;164;164m race\u001b[0m\u001b[38;2;255;171;171m no\u001b[0m\u001b[38;2;230;230;255m matter\u001b[0m\u001b[38;2;232;232;255m what\u001b[0m\u001b[38;2;238;238;255m it\u001b[0m\u001b[38;2;255;158;158m is\u001b[0m\u001b[38;2;255;255;255m!!\u001b[0m\n",
      "Activation Score: 0.53466796875\n",
      "\n",
      "Row 1:\n",
      "Text: The trans women reading this tweet right now is beautiful\n",
      "Activation Visualization:\n",
      "\u001b[38;2;248;248;255mThe\u001b[0m\u001b[38;2;255;214;214m trans\u001b[0m\u001b[38;2;255;203;203m women\u001b[0m\u001b[38;2;255;197;197m reading\u001b[0m\u001b[38;2;255;228;228m this\u001b[0m\u001b[38;2;255;201;201m twe\u001b[0m\u001b[38;2;255;236;236met\u001b[0m\u001b[38;2;255;203;203m right\u001b[0m\u001b[38;2;255;217;217m now\u001b[0m\u001b[38;2;255;121;121m is\u001b[0m\u001b[38;2;255;255;255m beautiful\u001b[0m\n",
      "Activation Score: -0.02252197265625\n",
      "\n",
      "Row 5:\n",
      "Text: Sounds like the kinda wholsesome life I'd die for ❤️ never met any black Lesbians before so i guess you're my first ever.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;229;229mSounds\u001b[0m\u001b[38;2;239;239;255m like\u001b[0m\u001b[38;2;255;235;235m the\u001b[0m\u001b[38;2;236;236;255m kinda\u001b[0m\u001b[38;2;220;220;255m wh\u001b[0m\u001b[38;2;229;229;255mol\u001b[0m\u001b[38;2;245;245;255mses\u001b[0m\u001b[38;2;255;233;233mome\u001b[0m\u001b[38;2;255;240;240m life\u001b[0m\u001b[38;2;243;243;255m I\u001b[0m\u001b[38;2;240;240;255m'\u001b[0m\u001b[38;2;232;232;255md\u001b[0m\u001b[38;2;0;0;255m die\u001b[0m\u001b[38;2;255;246;246m for\u001b[0m\u001b[38;2;250;250;255m \u001b[0m\u001b[38;2;204;204;255m❤\u001b[0m\u001b[38;2;247;247;255m️\u001b[0m\u001b[38;2;255;222;222m never\u001b[0m\u001b[38;2;255;231;231m met\u001b[0m\u001b[38;2;254;254;255m any\u001b[0m\u001b[38;2;225;225;255m black\u001b[0m\u001b[38;2;253;253;255m Les\u001b[0m\u001b[38;2;255;240;240mb\u001b[0m\u001b[38;2;239;239;255mians\u001b[0m\u001b[38;2;245;245;255m before\u001b[0m\u001b[38;2;211;211;255m so\u001b[0m\u001b[38;2;219;219;255m i\u001b[0m\u001b[38;2;219;219;255m guess\u001b[0m\u001b[38;2;255;247;247m you\u001b[0m\u001b[38;2;241;241;255m'\u001b[0m\u001b[38;2;234;234;255mre\u001b[0m\u001b[38;2;224;224;255m my\u001b[0m\u001b[38;2;220;220;255m first\u001b[0m\u001b[38;2;251;251;255m ever\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 0.61181640625\n",
      "\n",
      "Row 10:\n",
      "Text: 9) uhhhh i like being lgbt a lot. i feel proud of what i have done to help others in my community\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;253;253m9\u001b[0m\u001b[38;2;255;211;211m9\u001b[0m\u001b[38;2;255;217;217m)\u001b[0m\u001b[38;2;255;223;223m u\u001b[0m\u001b[38;2;245;245;255mhh\u001b[0m\u001b[38;2;255;243;243mhh\u001b[0m\u001b[38;2;255;198;198m i\u001b[0m\u001b[38;2;255;231;231m like\u001b[0m\u001b[38;2;234;234;255m being\u001b[0m\u001b[38;2;224;224;255m l\u001b[0m\u001b[38;2;242;242;255mg\u001b[0m\u001b[38;2;252;252;255mbt\u001b[0m\u001b[38;2;255;218;218m a\u001b[0m\u001b[38;2;255;179;179m lot\u001b[0m\u001b[38;2;225;225;255m.\u001b[0m\u001b[38;2;255;228;228m i\u001b[0m\u001b[38;2;255;204;204m feel\u001b[0m\u001b[38;2;255;219;219m proud\u001b[0m\u001b[38;2;255;229;229m of\u001b[0m\u001b[38;2;255;251;251m what\u001b[0m\u001b[38;2;255;222;222m i\u001b[0m\u001b[38;2;255;249;249m have\u001b[0m\u001b[38;2;253;253;255m done\u001b[0m\u001b[38;2;255;209;209m to\u001b[0m\u001b[38;2;255;169;169m help\u001b[0m\u001b[38;2;255;236;236m others\u001b[0m\u001b[38;2;255;225;225m in\u001b[0m\u001b[38;2;255;241;241m my\u001b[0m\u001b[38;2;255;255;255m community\u001b[0m\n",
      "Activation Score: 0.376708984375\n",
      "\n",
      "Row 12:\n",
      "Text: @terryelaineh1 @UKLabour Why do 3.8 million #50sWomen not constitute \"The Many\" not the few? Why is Labour not supporting women who've contributed all their working lives, paid for past pensions and NOW still paying, just for being women over 60! #BackTo60 #OneVoice #JudicialReview #CEDAW\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;242;242m@\u001b[0m\u001b[38;2;250;250;255mter\u001b[0m\u001b[38;2;240;240;255mry\u001b[0m\u001b[38;2;255;252;252mel\u001b[0m\u001b[38;2;252;252;255maine\u001b[0m\u001b[38;2;237;237;255mh\u001b[0m\u001b[38;2;255;239;239m1\u001b[0m\u001b[38;2;255;240;240m @\u001b[0m\u001b[38;2;255;234;234mUK\u001b[0m\u001b[38;2;250;250;255mL\u001b[0m\u001b[38;2;255;224;224mab\u001b[0m\u001b[38;2;255;202;202mour\u001b[0m\u001b[38;2;255;160;160m Why\u001b[0m\u001b[38;2;0;0;255m do\u001b[0m\u001b[38;2;255;222;222m \u001b[0m\u001b[38;2;252;252;255m3\u001b[0m\u001b[38;2;255;204;204m.\u001b[0m\u001b[38;2;255;220;220m8\u001b[0m\u001b[38;2;252;252;255m million\u001b[0m\u001b[38;2;224;224;255m #\u001b[0m\u001b[38;2;231;231;255m5\u001b[0m\u001b[38;2;255;253;253m0\u001b[0m\u001b[38;2;245;245;255ms\u001b[0m\u001b[38;2;255;241;241mW\u001b[0m\u001b[38;2;255;251;251momen\u001b[0m\u001b[38;2;255;252;252m not\u001b[0m\u001b[38;2;241;241;255m const\u001b[0m\u001b[38;2;255;247;247mitute\u001b[0m\u001b[38;2;219;219;255m \"\u001b[0m\u001b[38;2;255;227;227mThe\u001b[0m\u001b[38;2;255;211;211m Many\u001b[0m\u001b[38;2;255;209;209m\"\u001b[0m\u001b[38;2;255;244;244m not\u001b[0m\u001b[38;2;255;233;233m the\u001b[0m\u001b[38;2;255;171;171m few\u001b[0m\u001b[38;2;255;181;181m?\u001b[0m\u001b[38;2;255;169;169m Why\u001b[0m\u001b[38;2;255;184;184m is\u001b[0m\u001b[38;2;255;207;207m Labour\u001b[0m\u001b[38;2;255;186;186m not\u001b[0m\u001b[38;2;255;239;239m supporting\u001b[0m\u001b[38;2;255;238;238m women\u001b[0m\u001b[38;2;255;225;225m who\u001b[0m\u001b[38;2;250;250;255m'\u001b[0m\u001b[38;2;255;253;253mve\u001b[0m\u001b[38;2;226;226;255m contributed\u001b[0m\u001b[38;2;243;243;255m all\u001b[0m\u001b[38;2;240;240;255m their\u001b[0m\u001b[38;2;243;243;255m working\u001b[0m\u001b[38;2;255;240;240m lives\u001b[0m\u001b[38;2;255;240;240m,\u001b[0m\u001b[38;2;255;231;231m paid\u001b[0m\u001b[38;2;255;237;237m for\u001b[0m\u001b[38;2;255;251;251m past\u001b[0m\u001b[38;2;255;245;245m p\u001b[0m\u001b[38;2;227;227;255mensions\u001b[0m\u001b[38;2;255;243;243m and\u001b[0m\u001b[38;2;241;241;255m NO\u001b[0m\u001b[38;2;248;248;255mW\u001b[0m\u001b[38;2;255;254;254m still\u001b[0m\u001b[38;2;255;237;237m paying\u001b[0m\u001b[38;2;255;237;237m,\u001b[0m\u001b[38;2;255;243;243m just\u001b[0m\u001b[38;2;254;254;255m for\u001b[0m\u001b[38;2;224;224;255m being\u001b[0m\u001b[38;2;252;252;255m women\u001b[0m\u001b[38;2;234;234;255m over\u001b[0m\u001b[38;2;231;231;255m \u001b[0m\u001b[38;2;235;235;255m6\u001b[0m\u001b[38;2;255;224;224m0\u001b[0m\u001b[38;2;251;251;255m!\u001b[0m\u001b[38;2;255;222;222m #\u001b[0m\u001b[38;2;249;249;255mBack\u001b[0m\u001b[38;2;239;239;255mTo\u001b[0m\u001b[38;2;240;240;255m6\u001b[0m\u001b[38;2;242;242;255m0\u001b[0m\u001b[38;2;255;227;227m #\u001b[0m\u001b[38;2;245;245;255mOne\u001b[0m\u001b[38;2;254;254;255mV\u001b[0m\u001b[38;2;249;249;255moice\u001b[0m\u001b[38;2;245;245;255m #\u001b[0m\u001b[38;2;254;254;255mJ\u001b[0m\u001b[38;2;255;241;241mud\u001b[0m\u001b[38;2;255;254;254micial\u001b[0m\u001b[38;2;228;228;255mReview\u001b[0m\u001b[38;2;231;231;255m #\u001b[0m\u001b[38;2;251;251;255mC\u001b[0m\u001b[38;2;255;228;228mED\u001b[0m\u001b[38;2;255;255;255mAW\u001b[0m\n",
      "Activation Score: -0.09234619140625\n",
      "\n",
      "Row 15:\n",
      "Text: Ladies buy a gun learn how to use it effectively and kill these mother fuckers. URL\n",
      "Activation Visualization:\n",
      "\u001b[38;2;245;245;255mLad\u001b[0m\u001b[38;2;255;233;233mies\u001b[0m\u001b[38;2;255;225;225m buy\u001b[0m\u001b[38;2;249;249;255m a\u001b[0m\u001b[38;2;255;252;252m gun\u001b[0m\u001b[38;2;222;222;255m learn\u001b[0m\u001b[38;2;0;0;255m how\u001b[0m\u001b[38;2;218;218;255m to\u001b[0m\u001b[38;2;242;242;255m use\u001b[0m\u001b[38;2;219;219;255m it\u001b[0m\u001b[38;2;210;210;255m effectively\u001b[0m\u001b[38;2;240;240;255m and\u001b[0m\u001b[38;2;227;227;255m kill\u001b[0m\u001b[38;2;237;237;255m these\u001b[0m\u001b[38;2;208;208;255m mother\u001b[0m\u001b[38;2;222;222;255m fuck\u001b[0m\u001b[38;2;202;202;255mers\u001b[0m\u001b[38;2;255;229;229m.\u001b[0m\u001b[38;2;255;255;255m URL\u001b[0m\n",
      "Activation Score: 0.1865234375\n",
      "\n",
      "Row 19:\n",
      "Text: Progress 👍 LGBT rights in Europe: Some countries 'moving backwards on equality for first time in a decade' | Euronews URL #NewsSuite URL\n",
      "Activation Visualization:\n",
      "\u001b[38;2;0;0;255mProgress\u001b[0m\u001b[38;2;255;238;238m \u001b[0m\u001b[38;2;255;227;227m👍\u001b[0m\u001b[38;2;255;204;204m LGBT\u001b[0m\u001b[38;2;255;248;248m rights\u001b[0m\u001b[38;2;255;210;210m in\u001b[0m\u001b[38;2;255;213;213m Europe\u001b[0m\u001b[38;2;255;205;205m:\u001b[0m\u001b[38;2;255;229;229m Some\u001b[0m\u001b[38;2;255;201;201m countries\u001b[0m\u001b[38;2;255;234;234m '\u001b[0m\u001b[38;2;255;202;202mm\u001b[0m\u001b[38;2;246;246;255moving\u001b[0m\u001b[38;2;222;222;255m backwards\u001b[0m\u001b[38;2;255;199;199m on\u001b[0m\u001b[38;2;255;251;251m equality\u001b[0m\u001b[38;2;233;233;255m for\u001b[0m\u001b[38;2;222;222;255m first\u001b[0m\u001b[38;2;255;222;222m time\u001b[0m\u001b[38;2;255;253;253m in\u001b[0m\u001b[38;2;255;253;253m a\u001b[0m\u001b[38;2;248;248;255m decade\u001b[0m\u001b[38;2;255;206;206m'\u001b[0m\u001b[38;2;254;254;255m |\u001b[0m\u001b[38;2;255;253;253m E\u001b[0m\u001b[38;2;244;244;255mur\u001b[0m\u001b[38;2;255;242;242mone\u001b[0m\u001b[38;2;255;228;228mws\u001b[0m\u001b[38;2;226;226;255m URL\u001b[0m\u001b[38;2;240;240;255m #\u001b[0m\u001b[38;2;246;246;255mNews\u001b[0m\u001b[38;2;255;214;214mSuite\u001b[0m\u001b[38;2;255;255;255m URL\u001b[0m\n",
      "Activation Score: -0.19970703125\n",
      "\n",
      "Row 22:\n",
      "Text: No, they only do that to black people\n",
      "Activation Visualization:\n",
      "\u001b[38;2;0;0;255mNo\u001b[0m\u001b[38;2;242;242;255m,\u001b[0m\u001b[38;2;210;210;255m they\u001b[0m\u001b[38;2;183;183;255m only\u001b[0m\u001b[38;2;241;241;255m do\u001b[0m\u001b[38;2;186;186;255m that\u001b[0m\u001b[38;2;242;242;255m to\u001b[0m\u001b[38;2;230;230;255m black\u001b[0m\u001b[38;2;255;255;255m people\u001b[0m\n",
      "Activation Score: 0.884765625\n",
      "\n",
      "Row 23:\n",
      "Text: Many of the families who are being separated at the border and who are being kept in poor conditions did try to cross legally, through legal ports of entry seeking asylum.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;0;0;255mMany\u001b[0m\u001b[38;2;255;253;253m of\u001b[0m\u001b[38;2;238;238;255m the\u001b[0m\u001b[38;2;241;241;255m families\u001b[0m\u001b[38;2;234;234;255m who\u001b[0m\u001b[38;2;241;241;255m are\u001b[0m\u001b[38;2;211;211;255m being\u001b[0m\u001b[38;2;212;212;255m separated\u001b[0m\u001b[38;2;201;201;255m at\u001b[0m\u001b[38;2;234;234;255m the\u001b[0m\u001b[38;2;243;243;255m border\u001b[0m\u001b[38;2;250;250;255m and\u001b[0m\u001b[38;2;255;254;254m who\u001b[0m\u001b[38;2;255;246;246m are\u001b[0m\u001b[38;2;255;227;227m being\u001b[0m\u001b[38;2;255;244;244m kept\u001b[0m\u001b[38;2;255;232;232m in\u001b[0m\u001b[38;2;255;232;232m poor\u001b[0m\u001b[38;2;255;242;242m conditions\u001b[0m\u001b[38;2;232;232;255m did\u001b[0m\u001b[38;2;204;204;255m try\u001b[0m\u001b[38;2;207;207;255m to\u001b[0m\u001b[38;2;255;246;246m cross\u001b[0m\u001b[38;2;255;188;188m legally\u001b[0m\u001b[38;2;242;242;255m,\u001b[0m\u001b[38;2;255;254;254m through\u001b[0m\u001b[38;2;255;236;236m legal\u001b[0m\u001b[38;2;255;252;252m ports\u001b[0m\u001b[38;2;255;235;235m of\u001b[0m\u001b[38;2;255;247;247m entry\u001b[0m\u001b[38;2;230;230;255m seeking\u001b[0m\u001b[38;2;255;247;247m as\u001b[0m\u001b[38;2;255;221;221myl\u001b[0m\u001b[38;2;255;185;185mum\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: -0.39013671875\n",
      "\n",
      "Row 26:\n",
      "Text: Thank you for posting this! Gives us other \"normal\" women confidence. We all aren't skinny, or shaved, of perky. Doesn't mean we still aren't sexy.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;235;235mThank\u001b[0m\u001b[38;2;0;0;255m you\u001b[0m\u001b[38;2;255;222;222m for\u001b[0m\u001b[38;2;255;219;219m posting\u001b[0m\u001b[38;2;255;242;242m this\u001b[0m\u001b[38;2;255;251;251m!\u001b[0m\u001b[38;2;255;226;226m G\u001b[0m\u001b[38;2;255;224;224mives\u001b[0m\u001b[38;2;255;240;240m us\u001b[0m\u001b[38;2;221;221;255m other\u001b[0m\u001b[38;2;255;229;229m \"\u001b[0m\u001b[38;2;255;239;239mnormal\u001b[0m\u001b[38;2;255;245;245m\"\u001b[0m\u001b[38;2;255;211;211m women\u001b[0m\u001b[38;2;255;235;235m confidence\u001b[0m\u001b[38;2;244;244;255m.\u001b[0m\u001b[38;2;255;215;215m We\u001b[0m\u001b[38;2;255;174;174m all\u001b[0m\u001b[38;2;255;225;225m aren\u001b[0m\u001b[38;2;244;244;255m'\u001b[0m\u001b[38;2;230;230;255mt\u001b[0m\u001b[38;2;216;216;255m skin\u001b[0m\u001b[38;2;255;240;240mny\u001b[0m\u001b[38;2;204;204;255m,\u001b[0m\u001b[38;2;238;238;255m or\u001b[0m\u001b[38;2;255;221;221m sh\u001b[0m\u001b[38;2;255;245;245maved\u001b[0m\u001b[38;2;247;247;255m,\u001b[0m\u001b[38;2;246;246;255m of\u001b[0m\u001b[38;2;255;226;226m per\u001b[0m\u001b[38;2;255;153;153mky\u001b[0m\u001b[38;2;255;153;153m.\u001b[0m\u001b[38;2;255;132;132m Does\u001b[0m\u001b[38;2;255;184;184mn\u001b[0m\u001b[38;2;255;131;131m'\u001b[0m\u001b[38;2;255;137;137mt\u001b[0m\u001b[38;2;255;171;171m mean\u001b[0m\u001b[38;2;255;207;207m we\u001b[0m\u001b[38;2;255;235;235m still\u001b[0m\u001b[38;2;255;246;246m aren\u001b[0m\u001b[38;2;255;195;195m'\u001b[0m\u001b[38;2;255;164;164mt\u001b[0m\u001b[38;2;255;143;143m sexy\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 0.49560546875\n",
      "\n",
      "=====================================\n",
      "=====================================\n",
      "Activation visualization and scores for label 1 texts:\n",
      "Row 4:\n",
      "Text: For starters bend over the one in pink and kick that ass and pussy to get a taste until she's begging for a dick inside her.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;251;251;255mFor\u001b[0m\u001b[38;2;255;245;245m star\u001b[0m\u001b[38;2;202;202;255mters\u001b[0m\u001b[38;2;236;236;255m bend\u001b[0m\u001b[38;2;255;250;250m over\u001b[0m\u001b[38;2;249;249;255m the\u001b[0m\u001b[38;2;0;0;255m one\u001b[0m\u001b[38;2;243;243;255m in\u001b[0m\u001b[38;2;252;252;255m pink\u001b[0m\u001b[38;2;254;254;255m and\u001b[0m\u001b[38;2;255;224;224m kick\u001b[0m\u001b[38;2;245;245;255m that\u001b[0m\u001b[38;2;193;193;255m ass\u001b[0m\u001b[38;2;212;212;255m and\u001b[0m\u001b[38;2;234;234;255m p\u001b[0m\u001b[38;2;215;215;255mussy\u001b[0m\u001b[38;2;237;237;255m to\u001b[0m\u001b[38;2;243;243;255m get\u001b[0m\u001b[38;2;255;243;243m a\u001b[0m\u001b[38;2;232;232;255m taste\u001b[0m\u001b[38;2;199;199;255m until\u001b[0m\u001b[38;2;210;210;255m she\u001b[0m\u001b[38;2;212;212;255m'\u001b[0m\u001b[38;2;255;249;249ms\u001b[0m\u001b[38;2;253;253;255m begg\u001b[0m\u001b[38;2;242;242;255ming\u001b[0m\u001b[38;2;223;223;255m for\u001b[0m\u001b[38;2;248;248;255m a\u001b[0m\u001b[38;2;255;253;253m dick\u001b[0m\u001b[38;2;221;221;255m inside\u001b[0m\u001b[38;2;210;210;255m her\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 0.8857421875\n",
      "\n",
      "Row 7:\n",
      "Text: Fuck off you insufferable retarded faggot.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;234;234;255mFuck\u001b[0m\u001b[38;2;215;215;255m off\u001b[0m\u001b[38;2;213;213;255m you\u001b[0m\u001b[38;2;227;227;255m ins\u001b[0m\u001b[38;2;222;222;255muffer\u001b[0m\u001b[38;2;209;209;255mable\u001b[0m\u001b[38;2;162;162;255m ret\u001b[0m\u001b[38;2;198;198;255marded\u001b[0m\u001b[38;2;210;210;255m f\u001b[0m\u001b[38;2;189;189;255mag\u001b[0m\u001b[38;2;0;0;255mgot\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 4.90234375\n",
      "\n",
      "Row 17:\n",
      "Text: @suddenlywestan Because you are a country of mentally ill terrorists. Whoever says different is an enabler, ignorant and blind to endless/countless wars and aggression and the death rate by American terrorist acts. Fuck you American scum!\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;235;235m@\u001b[0m\u001b[38;2;255;249;249ms\u001b[0m\u001b[38;2;255;226;226mud\u001b[0m\u001b[38;2;255;249;249mdenly\u001b[0m\u001b[38;2;247;247;255mwest\u001b[0m\u001b[38;2;255;228;228man\u001b[0m\u001b[38;2;255;219;219m Because\u001b[0m\u001b[38;2;255;248;248m you\u001b[0m\u001b[38;2;255;254;254m are\u001b[0m\u001b[38;2;254;254;255m a\u001b[0m\u001b[38;2;0;0;255m country\u001b[0m\u001b[38;2;242;242;255m of\u001b[0m\u001b[38;2;222;222;255m mentally\u001b[0m\u001b[38;2;221;221;255m ill\u001b[0m\u001b[38;2;193;193;255m terror\u001b[0m\u001b[38;2;255;227;227mists\u001b[0m\u001b[38;2;248;248;255m.\u001b[0m\u001b[38;2;241;241;255m Who\u001b[0m\u001b[38;2;223;223;255mever\u001b[0m\u001b[38;2;255;210;210m says\u001b[0m\u001b[38;2;247;247;255m different\u001b[0m\u001b[38;2;243;243;255m is\u001b[0m\u001b[38;2;214;214;255m an\u001b[0m\u001b[38;2;213;213;255m en\u001b[0m\u001b[38;2;210;210;255mab\u001b[0m\u001b[38;2;255;243;243mler\u001b[0m\u001b[38;2;227;227;255m,\u001b[0m\u001b[38;2;199;199;255m ignorant\u001b[0m\u001b[38;2;255;252;252m and\u001b[0m\u001b[38;2;210;210;255m blind\u001b[0m\u001b[38;2;237;237;255m to\u001b[0m\u001b[38;2;195;195;255m endless\u001b[0m\u001b[38;2;225;225;255m/\u001b[0m\u001b[38;2;199;199;255mcount\u001b[0m\u001b[38;2;220;220;255mless\u001b[0m\u001b[38;2;202;202;255m wars\u001b[0m\u001b[38;2;233;233;255m and\u001b[0m\u001b[38;2;209;209;255m aggress\u001b[0m\u001b[38;2;228;228;255mion\u001b[0m\u001b[38;2;215;215;255m and\u001b[0m\u001b[38;2;226;226;255m the\u001b[0m\u001b[38;2;203;203;255m death\u001b[0m\u001b[38;2;216;216;255m rate\u001b[0m\u001b[38;2;255;252;252m by\u001b[0m\u001b[38;2;191;191;255m American\u001b[0m\u001b[38;2;212;212;255m terrorist\u001b[0m\u001b[38;2;255;249;249m acts\u001b[0m\u001b[38;2;255;227;227m.\u001b[0m\u001b[38;2;238;238;255m Fuck\u001b[0m\u001b[38;2;238;238;255m you\u001b[0m\u001b[38;2;201;201;255m American\u001b[0m\u001b[38;2;173;173;255m sc\u001b[0m\u001b[38;2;157;157;255mum\u001b[0m\u001b[38;2;255;255;255m!\u001b[0m\n",
      "Activation Score: 0.45751953125\n",
      "\n",
      "Row 24:\n",
      "Text: Get rid of them and their anchor babies and grand babies. Fucking worthless guinees\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;199;199mGet\u001b[0m\u001b[38;2;0;0;255m rid\u001b[0m\u001b[38;2;255;251;251m of\u001b[0m\u001b[38;2;255;241;241m them\u001b[0m\u001b[38;2;222;222;255m and\u001b[0m\u001b[38;2;238;238;255m their\u001b[0m\u001b[38;2;220;220;255m anchor\u001b[0m\u001b[38;2;226;226;255m babies\u001b[0m\u001b[38;2;251;251;255m and\u001b[0m\u001b[38;2;233;233;255m grand\u001b[0m\u001b[38;2;205;205;255m babies\u001b[0m\u001b[38;2;234;234;255m.\u001b[0m\u001b[38;2;207;207;255m Fuck\u001b[0m\u001b[38;2;242;242;255ming\u001b[0m\u001b[38;2;187;187;255m worth\u001b[0m\u001b[38;2;185;185;255mless\u001b[0m\u001b[38;2;255;246;246m gu\u001b[0m\u001b[38;2;239;239;255mine\u001b[0m\u001b[38;2;255;255;255mes\u001b[0m\n",
      "Activation Score: 0.473388671875\n",
      "\n",
      "Row 30:\n",
      "Text: @KeGutta These niggas straight bitches\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;239;239m@\u001b[0m\u001b[38;2;251;251;255mKe\u001b[0m\u001b[38;2;255;250;250mG\u001b[0m\u001b[38;2;255;250;250mut\u001b[0m\u001b[38;2;255;246;246mta\u001b[0m\u001b[38;2;255;230;230m These\u001b[0m\u001b[38;2;244;244;255m n\u001b[0m\u001b[38;2;213;213;255mig\u001b[0m\u001b[38;2;255;245;245mgas\u001b[0m\u001b[38;2;230;230;255m straight\u001b[0m\u001b[38;2;255;254;254m bit\u001b[0m\u001b[38;2;255;255;255mches\u001b[0m\n",
      "Activation Score: 0.69873046875\n",
      "\n",
      "Row 37:\n",
      "Text: Dwight Howard is a undercover Faggot, this guy was caught on camera fondling Isiah Canaan penis when you were playing for the Rockets. Dwight's career is over because he's softer than marmalade and he couldn't shoot a beach ball into a ocean.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;232;232mD\u001b[0m\u001b[38;2;240;240;255mw\u001b[0m\u001b[38;2;255;251;251might\u001b[0m\u001b[38;2;255;235;235m Howard\u001b[0m\u001b[38;2;255;247;247m is\u001b[0m\u001b[38;2;246;246;255m a\u001b[0m\u001b[38;2;240;240;255m under\u001b[0m\u001b[38;2;233;233;255mcover\u001b[0m\u001b[38;2;207;207;255m F\u001b[0m\u001b[38;2;187;187;255mag\u001b[0m\u001b[38;2;0;0;255mgot\u001b[0m\u001b[38;2;232;232;255m,\u001b[0m\u001b[38;2;223;223;255m this\u001b[0m\u001b[38;2;250;250;255m guy\u001b[0m\u001b[38;2;236;236;255m was\u001b[0m\u001b[38;2;214;214;255m caught\u001b[0m\u001b[38;2;232;232;255m on\u001b[0m\u001b[38;2;228;228;255m camera\u001b[0m\u001b[38;2;251;251;255m fond\u001b[0m\u001b[38;2;244;244;255mling\u001b[0m\u001b[38;2;255;239;239m Is\u001b[0m\u001b[38;2;235;235;255miah\u001b[0m\u001b[38;2;252;252;255m C\u001b[0m\u001b[38;2;255;236;236mana\u001b[0m\u001b[38;2;192;192;255man\u001b[0m\u001b[38;2;205;205;255m penis\u001b[0m\u001b[38;2;224;224;255m when\u001b[0m\u001b[38;2;255;241;241m you\u001b[0m\u001b[38;2;255;211;211m were\u001b[0m\u001b[38;2;255;235;235m playing\u001b[0m\u001b[38;2;255;244;244m for\u001b[0m\u001b[38;2;254;254;255m the\u001b[0m\u001b[38;2;255;202;202m Rock\u001b[0m\u001b[38;2;224;224;255mets\u001b[0m\u001b[38;2;209;209;255m.\u001b[0m\u001b[38;2;236;236;255m D\u001b[0m\u001b[38;2;228;228;255mw\u001b[0m\u001b[38;2;201;201;255might\u001b[0m\u001b[38;2;190;190;255m'\u001b[0m\u001b[38;2;255;198;198ms\u001b[0m\u001b[38;2;255;252;252m career\u001b[0m\u001b[38;2;255;238;238m is\u001b[0m\u001b[38;2;223;223;255m over\u001b[0m\u001b[38;2;185;185;255m because\u001b[0m\u001b[38;2;188;188;255m he\u001b[0m\u001b[38;2;182;182;255m'\u001b[0m\u001b[38;2;215;215;255ms\u001b[0m\u001b[38;2;222;222;255m so\u001b[0m\u001b[38;2;226;226;255mfter\u001b[0m\u001b[38;2;213;213;255m than\u001b[0m\u001b[38;2;229;229;255m m\u001b[0m\u001b[38;2;241;241;255marm\u001b[0m\u001b[38;2;250;250;255mal\u001b[0m\u001b[38;2;211;211;255made\u001b[0m\u001b[38;2;221;221;255m and\u001b[0m\u001b[38;2;186;186;255m he\u001b[0m\u001b[38;2;249;249;255m couldn\u001b[0m\u001b[38;2;244;244;255m'\u001b[0m\u001b[38;2;247;247;255mt\u001b[0m\u001b[38;2;214;214;255m shoot\u001b[0m\u001b[38;2;224;224;255m a\u001b[0m\u001b[38;2;246;246;255m beach\u001b[0m\u001b[38;2;212;212;255m ball\u001b[0m\u001b[38;2;210;210;255m into\u001b[0m\u001b[38;2;211;211;255m a\u001b[0m\u001b[38;2;255;250;250m ocean\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 0.69091796875\n",
      "\n",
      "Row 48:\n",
      "Text: @odysseuslahori @IqShoaib Hang till death all religious pimps !!\n",
      "Activation Visualization:\n",
      "\u001b[38;2;255;238;238m@\u001b[0m\u001b[38;2;251;251;255mod\u001b[0m\u001b[38;2;255;239;239mys\u001b[0m\u001b[38;2;244;244;255mse\u001b[0m\u001b[38;2;250;250;255mus\u001b[0m\u001b[38;2;254;254;255ml\u001b[0m\u001b[38;2;255;254;254mah\u001b[0m\u001b[38;2;255;233;233mori\u001b[0m\u001b[38;2;245;245;255m @\u001b[0m\u001b[38;2;242;242;255mI\u001b[0m\u001b[38;2;255;248;248mq\u001b[0m\u001b[38;2;255;242;242mSh\u001b[0m\u001b[38;2;255;236;236moa\u001b[0m\u001b[38;2;255;246;246mib\u001b[0m\u001b[38;2;233;233;255m H\u001b[0m\u001b[38;2;255;238;238mang\u001b[0m\u001b[38;2;255;218;218m till\u001b[0m\u001b[38;2;247;247;255m death\u001b[0m\u001b[38;2;255;192;192m all\u001b[0m\u001b[38;2;253;253;255m religious\u001b[0m\u001b[38;2;235;235;255m p\u001b[0m\u001b[38;2;255;252;252mim\u001b[0m\u001b[38;2;234;234;255mps\u001b[0m\u001b[38;2;255;255;255m !!\u001b[0m\n",
      "Activation Score: 0.59228515625\n",
      "\n",
      "Row 50:\n",
      "Text: Saudi should attack Iran and see what happens to these barbaric cowards! America should stay out of the fight.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;251;251;255mSaudi\u001b[0m\u001b[38;2;245;245;255m should\u001b[0m\u001b[38;2;255;252;252m attack\u001b[0m\u001b[38;2;0;0;255m Iran\u001b[0m\u001b[38;2;237;237;255m and\u001b[0m\u001b[38;2;249;249;255m see\u001b[0m\u001b[38;2;255;252;252m what\u001b[0m\u001b[38;2;255;234;234m happens\u001b[0m\u001b[38;2;255;230;230m to\u001b[0m\u001b[38;2;222;222;255m these\u001b[0m\u001b[38;2;209;209;255m bar\u001b[0m\u001b[38;2;220;220;255mbar\u001b[0m\u001b[38;2;199;199;255mic\u001b[0m\u001b[38;2;203;203;255m cow\u001b[0m\u001b[38;2;209;209;255mards\u001b[0m\u001b[38;2;245;245;255m!\u001b[0m\u001b[38;2;250;250;255m America\u001b[0m\u001b[38;2;255;248;248m should\u001b[0m\u001b[38;2;255;223;223m stay\u001b[0m\u001b[38;2;255;238;238m out\u001b[0m\u001b[38;2;223;223;255m of\u001b[0m\u001b[38;2;207;207;255m the\u001b[0m\u001b[38;2;234;234;255m fight\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 0.70166015625\n",
      "\n",
      "Row 55:\n",
      "Text: Niggers are ruining this country\n",
      "Activation Visualization:\n",
      "\u001b[38;2;204;204;255mN\u001b[0m\u001b[38;2;172;172;255miggers\u001b[0m\u001b[38;2;248;248;255m are\u001b[0m\u001b[38;2;242;242;255m ru\u001b[0m\u001b[38;2;255;240;240mining\u001b[0m\u001b[38;2;243;243;255m this\u001b[0m\u001b[38;2;255;255;255m country\u001b[0m\n",
      "Activation Score: 0.7802734375\n",
      "\n",
      "Row 68:\n",
      "Text: Islam needs to be fucking destroyed.\n",
      "Activation Visualization:\n",
      "\u001b[38;2;252;252;255mIslam\u001b[0m\u001b[38;2;0;0;255m needs\u001b[0m\u001b[38;2;255;232;232m to\u001b[0m\u001b[38;2;239;239;255m be\u001b[0m\u001b[38;2;227;227;255m fucking\u001b[0m\u001b[38;2;209;209;255m destroyed\u001b[0m\u001b[38;2;255;255;255m.\u001b[0m\n",
      "Activation Score: 1.09375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter 10 texts with label 0 and 10 texts with label 1\n",
    "label_0_texts = df[df['label'] == 0].head(10)\n",
    "label_1_texts = df[df['label'] == 1].head(10)\n",
    "print(len(label_1_texts))\n",
    "# Process and print activation visualization and scores for label 0 texts\n",
    "print(\"Activation visualization and scores for label 0 texts:\")\n",
    "for i, row in label_0_texts.iterrows():\n",
    "    text = row['text']\n",
    "    activation_visualization = model.visualize_activation(input_text=text, control_vector=vector)\n",
    "    activation_score = model.get_activation_score(text, vector, layer_index=20)\n",
    "    print(f\"Row {i}:\\nText: {text}\\nActivation Visualization:\\n{activation_visualization}\\nActivation Score: {activation_score}\\n\")\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "# Process and print activation visualization and scores for label 1 texts\n",
    "print(\"Activation visualization and scores for label 1 texts:\")\n",
    "for i, row in label_1_texts.iterrows():\n",
    "    text = row['text']\n",
    "    activation_visualization = model.visualize_activation(input_text=text, control_vector=vector)\n",
    "    activation_score = model.get_activation_score(text, vector, layer_index=20)\n",
    "    print(f\"Row {i}:\\nText: {text}\\nActivation Visualization:\\n{activation_visualization}\\nActivation Score: {activation_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==baseline ---------------------------------------------------\n",
      "<s> Write about a black man.\n",
      "A black man is often stereotyped as being strong, powerful and aggressive. However, like all individuals, he is complex with a range of emotions, experiences and perspectives. He may face challenges and discrimination due to his race, but his resilience and determination can overcome these obst\n",
      "\n",
      "++control ---------------------------------------------------\n",
      "<s> Write about a black man.\n",
      "Yes, all black men are black yes but they can be all different types of black yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes\n",
      "\n",
      "--control ---------------------------------------------------\n",
      "<s> Write about a black man.\n",
      "\n",
      "A black man is a person who does not discriminate against others based on their race or ethnicity. He is a man who has experienced discrimination and prejudice, but never allows such feelings to cloud his judgment or thoughts. Instead, he chooses to judge people by their character and actions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "list_of_strings = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=model.token)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "def generate_with_vector(\n",
    "    input: str,\n",
    "    vector: ControlVector,\n",
    "    coeffs: tuple[float, float],\n",
    "    max_new_tokens: int = 20,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    show_baseline: bool = True,\n",
    "):\n",
    "    positive_coeff, negative_coeff = coeffs\n",
    "    assert positive_coeff > 0\n",
    "    assert negative_coeff < 0\n",
    "\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").to(model.device)\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "        \"do_sample\": False,  # temperature=0\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    if show_baseline:\n",
    "        print(\"==baseline ---------------------------------------------------\")\n",
    "        model.reset()\n",
    "        print(\n",
    "            tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "        )\n",
    "        list_of_strings.append(\n",
    "            tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "        )\n",
    "\n",
    "    print(\"\\n++control ---------------------------------------------------\")\n",
    "    model.set_control(vector, positive_coeff)\n",
    "    print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip())\n",
    "    list_of_strings.append(\n",
    "        tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "    )\n",
    "\n",
    "    print(\"\\n--control ---------------------------------------------------\")\n",
    "    model.set_control(vector, negative_coeff)\n",
    "    print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip())\n",
    "    list_of_strings.append(\n",
    "        tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "    )\n",
    "\n",
    "    model.reset()\n",
    "\n",
    "    with open(f\"output_{input[:10]}.txt\", \"w\") as f:\n",
    "        for string in list_of_strings:\n",
    "            f.write(string + \"\\n\")\n",
    "\n",
    "\n",
    "generate_with_vector(\"Write about a black man.\", vector, (2,-2), 60)\n",
    "\n",
    "\n",
    "\n",
    "# highlighted_output = model.visualize_activation(\n",
    "#     input_text=\"Good CEOs are all men. Men tend to be better at leadership roles.\",\n",
    "#     control_vector=vector,\n",
    "# )\n",
    "\n",
    "# print(highlighted_output)\n",
    "\n",
    "# highlighted_output = model.visualize_activation(\n",
    "#     input_text=\"Women are supposed to stay in the kitchen not go out and become nurses.\",\n",
    "#     control_vector=vector,\n",
    "# )\n",
    "# print(highlighted_output)\n",
    "\n",
    "# highlighted_output = model.visualize_activation(\n",
    "#     input_text=\"Mothers are the most amazing thing to bless humankind.\",\n",
    "#     control_vector=vector,\n",
    "# )\n",
    "# print(highlighted_output)\n",
    "\n",
    "# print(\n",
    "#     model.get_activation_score(\"Female empowerment is an amazing thing.\", vector)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
